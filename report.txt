Michael Krumdick

HOW TO RUN:
$ make
$ ./neural iris.txt
		or
$ ./neural cancer.txt

WHAT WILL HAPPEN:
 The program will display its final set of classificaitons and its average success rate
 over all of its tests. It may take a few seconds. This average value should be about 93%
 for iris.txt and 89% for cancer.txt.

WHAT ARE THE TWO DATA SETS:

iris.txt-
	The iris flower data set. This data set, commonly used in machine learning, contains
	the measurements of various parts of a flower and its species. The program identifies
	which type of flower matches the data.

cancer.txt-
	This dataset contains information about breast cancer tumors. It predicts whether or
	not they are malignant. Data from:
	https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29

DIRECTIONS FOR ADDING ANOTHER DATASET:
	If you wished to use this program for another data set, on the first line of
	file write how many values that each point has, along with the number of possible
	classifications seperated by a space. The data points need to be real numbers. 

WHAT IS HAPPENING:
	First the program reads the .txt file. It seperates the string of data using a strtok
	function. It then takes these values and puts them in a custom struct called datap_t.
	I used linked lists because I wanted variable memory and I didn't anticipate having
	to ever acces a specific index without accessing every index. Once it has the data,
	it scales every value to bewteen one and two, to make sure that the inital weighting
	of the values is equal. It then runs the Aggregate Network function. This function
	simply takes the weights of a bunch of individual networks to give an accurate 
	assessment of its learning ability. The data is randomized before being passed into
	the network. To randomize the data, I put the list into an array and did a 
	Fisher-Yates shuffle to swap around the data points. It is also split into two 
	groups, the learning group and the test group. The program learns the patterns from
	the learning group and tests them on the test group. This is done to prevent
	overfitting to the learning data. When the data is in its proper form, it is passed
	into a network function. This function is the brain. It is what learns the patterns
	and creates the weights. To do this, it first declares and array of weights. It 
	computes the sum of the weighted data points and puts this through a sigmoid 
	function. It classifies the data by dividing up the range from 0 to 1 into various 
	boxes. The target value for each classifications is defined by the center point
	of this box. To calculate the error then, I took the difference between the	
	computed value and the target value. I then used error back propagation to alter the
	weights on the data values. Error backpropagation is hard to explain in text, but 
	basically you are measuring how far off the total value is, and then calculating 
	the change in the weight using this value and the derivative of the function. I do
	this over the set of data a bunch of times to generate more and more accurate 
	weights. The to find the success rate of the individual network,I go through the test
	data with the learned weights and find how many are put into the right "box." The 
	then goes through all the data one last time toprint out an example classificaiton
	set to give a visual representation of how succesful it is. It then displays that 
	average rate that I found from earlier. 
